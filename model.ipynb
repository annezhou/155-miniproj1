{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS155: Miniproject 1\n",
    "Kavya Sreedhar, Audrey Wang, Anne Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Define function for loading files\n",
    "def load_data(filename, skiprows=1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "train = load_data('training_data.txt')\n",
    "X_test = load_data('test_data.txt')\n",
    "\n",
    "X_train = train[:, 1:]\n",
    "y_train = train[:, 0]\n",
    "N_train = len(X_train)\n",
    "N_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize data.\n",
    "max_vals = X_train.max(axis=0)\n",
    "X_train = X_train / max_vals\n",
    "X_test = X_test / max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm: 0.530500\n",
      "[ 0.53073463  0.5303826   0.5303826 ]\n",
      "\n",
      "logistic regression: 0.829997\n",
      "[ 0.84332834  0.8372093   0.80945236]\n",
      "\n",
      "random forest: 0.753246\n",
      "[ 0.76911544  0.76294074  0.72768192]\n",
      "\n",
      "gradient boost: 0.786494\n",
      "[ 0.81034483  0.79969992  0.74943736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find cross-validation score of different models to determine best one.\n",
    "X_val = X_train[:4000]\n",
    "y_val = y_train[:4000]\n",
    "types = ['svm', 'logistic regression', 'random forest', 'gradient boost']\n",
    "scores = []\n",
    "\n",
    "sv = svm.SVC()\n",
    "log = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "scores.append(cross_val_score(sv, X_val, y_val))\n",
    "scores.append(cross_val_score(log, X_val, y_val))\n",
    "scores.append(cross_val_score(random_forest, X_val, y_val))\n",
    "scores.append(cross_val_score(gradient_boost, X_val, y_val))\n",
    "\n",
    "for i in range(len(types)):\n",
    "    print('%s: %f' % (types[i], np.mean(scores[i])))\n",
    "    print(scores[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only balanced: 0.847000\n",
      "[ 0.84970751  0.84490775  0.84638464]\n",
      "0.3\n",
      "only c: 0.846350\n",
      "[ 0.84865757  0.84325784  0.84713471]\n",
      "both c and balanced: 0.846500\n",
      "[ 0.85015749  0.84280786  0.84653465]\n",
      "\n",
      "0.5\n",
      "only c: 0.846800\n",
      "[ 0.84940753  0.84460777  0.84638464]\n",
      "both c and balanced: 0.847500\n",
      "[ 0.84985751  0.84460777  0.8480348 ]\n",
      "\n",
      "0.8\n",
      "only c: 0.846250\n",
      "[ 0.84925754  0.84415779  0.84533453]\n",
      "both c and balanced: 0.846750\n",
      "[ 0.84865757  0.84580771  0.84578458]\n",
      "\n",
      "1.0\n",
      "only c: 0.846100\n",
      "[ 0.84940753  0.84310784  0.84578458]\n",
      "both c and balanced: 0.847000\n",
      "[ 0.84970751  0.84490775  0.84638464]\n",
      "\n",
      "2.0\n",
      "only c: 0.846700\n",
      "[ 0.84970751  0.84445778  0.84593459]\n",
      "both c and balanced: 0.847300\n",
      "[ 0.84955752  0.84520774  0.84713471]\n",
      "\n",
      "5.0\n",
      "only c: 0.845450\n",
      "[ 0.84580771  0.84370781  0.84683468]\n",
      "both c and balanced: 0.845550\n",
      "[ 0.84610769  0.84355782  0.8469847 ]\n",
      "\n",
      "10.0\n",
      "only c: 0.843050\n",
      "[ 0.84355782  0.84040798  0.84518452]\n",
      "both c and balanced: 0.843250\n",
      "[ 0.84355782  0.84025799  0.84593459]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tweak parameters for best classifiers.\n",
    "c_arr = [0.3, 0.5, 0.8, 1.0, 2.0, 5.0, 10.0]\n",
    "log1 = LogisticRegression(class_weight='balanced')\n",
    "score1 = cross_val_score(log1, X_train, y_train)\n",
    "print('only balanced: %f' % np.mean(score1))\n",
    "print(score1)\n",
    "\n",
    "for i in range(len(c_arr)):\n",
    "    print(c_arr[i])\n",
    "    log2 = LogisticRegression(C=c_arr[i])\n",
    "    score2 = cross_val_score(log2, X_train, y_train)\n",
    "    print('only c: %f' % np.mean(score2))\n",
    "    print(score2)\n",
    "    log4 = LogisticRegression(C=c_arr[i], class_weight='balanced')\n",
    "    score4 = cross_val_score(log4, X_train, y_train)\n",
    "    print('both c and balanced: %f' % np.mean(score4))\n",
    "    print(score4)\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions to output file.\n",
    "clf = LogisticRegression(C=2.0, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test).flatten()\n",
    "f = open('predictions.txt', 'w')\n",
    "f.write('Id,Prediction\\n')\n",
    "for i in range(len(predictions)):\n",
    "    f.write('%d,%d\\n' % ((i + 1), predictions[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "# from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 13s 632us/step - loss: 0.2573 - acc: 0.7679\n",
      "20000/20000 [==============================] - 2s 120us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.074854408216476445, 0.92405000000000004]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N = 1000 # Number of parameters\n",
    "\n",
    "# # Define the model.\n",
    "# model = Sequential()\n",
    "# model.add(Dense(1000, input_shape=(N,)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(900))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dense(800))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(10))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# # Print number of params\n",
    "# model.count_params()\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='mse',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model for 1 epoch\n",
    "# history = model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.evaluate(x=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
